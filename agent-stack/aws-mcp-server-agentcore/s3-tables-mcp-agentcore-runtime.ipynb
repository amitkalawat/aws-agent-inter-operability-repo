{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3 Tables MCP Server on AgentCore Runtime\n",
    "\n",
    "This notebook demonstrates how to test and deploy the AWS S3 Tables MCP server to Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "## Prerequisites Setup\n",
    "\n",
    "**Step 1:** Clone the AWS MCP repository\n",
    "```bash\n",
    "git clone https://github.com/awslabs/mcp.git\n",
    "```\n",
    "\n",
    "**Step 2:** Copy the AWS S3 Tables MCP server to your project root\n",
    "```bash\n",
    "cp -r ./mcp/src/s3-tables-mcp-server ./\n",
    "```\n",
    "\n",
    "**Step 3:** Set up your environment variables in `.env` file:\n",
    "```\n",
    "COGNITO_POOL_ID=your_pool_id\n",
    "COGNITO_REGION=us-east-1\n",
    "COGNITO_USERNAME=admin\n",
    "COGNITO_CLIENT_SECRET=your_client_secret\n",
    "COGNITO_PASSWORD=your_password\n",
    "AWS_PROFILE=default\n",
    "```\n",
    "\n",
    "**Step 4:** Follow the instructions below to complete the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS S3 Tables MCP Server Overview\n",
    "\n",
    "The AWS S3 Tables MCP server provides comprehensive S3-based table storage management capabilities through 16 specialized tools:\n",
    "\n",
    "### Table Bucket Management Tools\n",
    "- **`list_table_buckets`**: List all S3 table buckets in your AWS account\n",
    "- **`create_table_bucket`**: Create new S3 Table Buckets to organize tabular data at scale\n",
    "- **`get_bucket_metadata_config`**: Get metadata table configuration for regular S3 buckets\n",
    "\n",
    "### Namespace Management Tools\n",
    "- **`list_namespaces`**: List all namespaces across all S3 table buckets\n",
    "- **`create_namespace`**: Create namespaces within table buckets for logical data separation\n",
    "\n",
    "### Table Management Tools\n",
    "- **`list_tables`**: List all S3 tables across all table buckets and namespaces\n",
    "- **`create_table`**: Create individual tables within namespaces with flexible schema definition\n",
    "- **`rename_table`**: Rename tables or move them to different namespaces\n",
    "- **`get_table_metadata_location`**: Get the S3 URI location of table metadata\n",
    "- **`update_table_metadata_location`**: Update the metadata location for S3 tables\n",
    "\n",
    "### Maintenance and Monitoring Tools\n",
    "- **`get_table_maintenance_config`**: Retrieve maintenance settings for tables\n",
    "- **`get_maintenance_job_status`**: Get the status of maintenance jobs for tables\n",
    "\n",
    "### Data Operations Tools\n",
    "- **`query_database`**: Execute read-only SQL queries against S3 Tables using PyIceberg/Daft\n",
    "- **`import_csv_to_table`**: Create S3 Tables from CSV files uploaded to S3\n",
    "- **`import_parquet_to_table`**: Create S3 Tables from Parquet files uploaded to S3\n",
    "- **`append_rows_to_table`**: Append new data rows to existing S3 tables\n",
    "\n",
    "### Key Features\n",
    "- **Read-Only Mode**: Optional security mode that restricts all operations to read-only\n",
    "- **SQL Query Support**: Execute read-only SQL queries with insert-only write operations\n",
    "- **Automatic Schema Inference**: Create tables from CSV/Parquet files with inferred schemas\n",
    "- **Metadata Discovery**: Access comprehensive bucket metadata through S3 Metadata Tables\n",
    "- **Flexible Data Modeling**: Support for Apache Iceberg format with partitioning\n",
    "\n",
    "All tools work with S3-based table storage and require appropriate AWS S3 Tables permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "### System Requirements\n",
    "- Python 3.10 or higher\n",
    "- AWS CLI configured with valid credentials\n",
    "- Docker installed (for containerization)\n",
    "\n",
    "### AWS Permissions\n",
    "Your AWS credentials must have permissions for:\n",
    "- AWS S3 Tables (full access recommended)\n",
    "- Amazon S3 (for data storage and retrieval)\n",
    "- Amazon Bedrock AgentCore\n",
    "- Amazon ECR (for container registry)\n",
    "- Amazon Cognito (for authentication)\n",
    "- IAM (for role creation)\n",
    "- AWS Systems Manager Parameter Store\n",
    "- AWS Secrets Manager\n",
    "\n",
    "### Project Structure\n",
    "- `s3-tables-mcp-server/` - The MCP server implementation\n",
    "- `requirements.txt` - Python dependencies\n",
    "- `utils.py` - Helper functions for Cognito and IAM setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install all required Python packages using uv (recommended) or pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r s3-table-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local Testing\n",
    "\n",
    "Before deploying to AgentCore Runtime, let's test the AWS S3 Tables MCP server locally.\n",
    "\n",
    "### 2.1 MCP Server Wrapper\n",
    "\n",
    "The `mcp-server.py` file creates a FastMCP wrapper around the AWS S3 Tables MCP server for AgentCore deployment. The reason we have to do this is because AWS MCP servers are implemented for Stdio local run, we are extending it for remote MCP server using this wrapper:\n",
    "\n",
    "- Imports all 16 tools from the original server implementation\n",
    "- Configures the server for HTTP transport on port 8000\n",
    "- Sets up proper instructions and dependencies\n",
    "- Enables stateless HTTP mode for AgentCore compatibility\n",
    "\n",
    "The server exposes these tools:\n",
    "- Table bucket management: `list_table_buckets`, `create_table_bucket`, `get_bucket_metadata_config`\n",
    "- Namespace management: `list_namespaces`, `create_namespace`\n",
    "- Table management: `list_tables`, `create_table`, `rename_table`, `get_table_metadata_location`, `update_table_metadata_location`\n",
    "- Maintenance: `get_table_maintenance_config`, `get_maintenance_job_status`\n",
    "- Data operations: `query_database`, `import_csv_to_table`, `import_parquet_to_table`, `append_rows_to_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp-server.py\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the path before any imports\n",
    "sys.path.insert(0, os.path.abspath(\"./s3-tables-mcp-server\"))\n",
    "\n",
    "# Import the tool functions from the original server\n",
    "from awslabs.s3_tables_mcp_server.server import (\n",
    "    list_table_buckets, create_table_bucket, get_bucket_metadata_config,\n",
    "    list_namespaces, create_namespace,\n",
    "    list_tables, create_table, rename_table, get_table_metadata_location, update_table_metadata_location,\n",
    "    get_table_maintenance_config, get_maintenance_job_status,\n",
    "    query_database, import_csv_to_table, import_parquet_to_table, append_rows_to_table\n",
    ")\n",
    "\n",
    "# Create a new FastMCP instance with correct parameters\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\n",
    "    'awslabs.s3-tables-mcp-server',\n",
    "    host=\"0.0.0.0\",\n",
    "    stateless_http=True,\n",
    "    instructions=\"\"\"AWS S3 Tables MCP Server provides comprehensive tools for managing S3-based table storage through Amazon S3 Tables.\n",
    "\n",
    "    This server enables you to:\n",
    "    - Create and manage S3 Table Buckets for organizing tabular data at scale\n",
    "    - Define namespaces within table buckets for logical data separation\n",
    "    - Create, rename, and manage individual S3 tables with flexible schema definition\n",
    "    - Execute read-only SQL queries against S3 Tables using PyIceberg/Daft engine\n",
    "    - Import data from CSV and Parquet files stored in S3\n",
    "    - Append new data rows to existing tables\n",
    "    - Access comprehensive metadata and maintenance configurations\n",
    "    - Discover bucket metadata through S3 Metadata Tables\n",
    "\n",
    "    ## Available Tools:\n",
    "    \n",
    "    ### Table Bucket Management\n",
    "    - list_table_buckets: List all S3 table buckets in your account\n",
    "    - create_table_bucket: Create new table buckets for data organization\n",
    "    - get_bucket_metadata_config: Get metadata configuration for S3 buckets\n",
    "    \n",
    "    ### Namespace Management\n",
    "    - list_namespaces: List all namespaces across table buckets\n",
    "    - create_namespace: Create logical groupings within table buckets\n",
    "    \n",
    "    ### Table Management\n",
    "    - list_tables: List all tables across buckets and namespaces\n",
    "    - create_table: Create tables with Apache Iceberg format and custom schemas\n",
    "    - rename_table: Rename tables or move them between namespaces\n",
    "    - get_table_metadata_location: Get S3 URI of table metadata\n",
    "    - update_table_metadata_location: Update metadata location for tables\n",
    "    \n",
    "    ### Maintenance and Monitoring\n",
    "    - get_table_maintenance_config: Retrieve table maintenance settings\n",
    "    - get_maintenance_job_status: Monitor maintenance job status\n",
    "    \n",
    "    ### Data Operations\n",
    "    - query_database: Execute SQL queries against S3 Tables (read-only with insert support)\n",
    "    - import_csv_to_table: Create tables from CSV files with schema inference\n",
    "    - import_parquet_to_table: Create tables from Parquet files with schema inference\n",
    "    - append_rows_to_table: Add new data rows to existing tables\n",
    "\n",
    "    The server supports both read-only and write operations, with optional security modes for production use.\n",
    "\n",
    "    For more information about AWS S3 Tables, visit:\n",
    "    https://aws.amazon.com/s3/features/tables/\n",
    "    \"\"\",\n",
    "    dependencies=[\n",
    "        'pydantic',\n",
    "        'loguru',\n",
    "        'boto3',\n",
    "        'pyiceberg',\n",
    "        'daft-io',\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Register all the tools from the original server\n",
    "mcp.tool(name='list_table_buckets')(list_table_buckets)\n",
    "mcp.tool(name='create_table_bucket')(create_table_bucket)\n",
    "mcp.tool(name='get_bucket_metadata_config')(get_bucket_metadata_config)\n",
    "mcp.tool(name='list_namespaces')(list_namespaces)\n",
    "mcp.tool(name='create_namespace')(create_namespace)\n",
    "mcp.tool(name='list_tables')(list_tables)\n",
    "mcp.tool(name='create_table')(create_table)\n",
    "mcp.tool(name='rename_table')(rename_table)\n",
    "mcp.tool(name='get_table_metadata_location')(get_table_metadata_location)\n",
    "mcp.tool(name='update_table_metadata_location')(update_table_metadata_location)\n",
    "mcp.tool(name='get_table_maintenance_config')(get_table_maintenance_config)\n",
    "mcp.tool(name='get_maintenance_job_status')(get_maintenance_job_status)\n",
    "mcp.tool(name='query_database')(query_database)\n",
    "mcp.tool(name='import_csv_to_table')(import_csv_to_table)\n",
    "mcp.tool(name='import_parquet_to_table')(import_parquet_to_table)\n",
    "mcp.tool(name='append_rows_to_table')(append_rows_to_table)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting MCP server in HTTP mode on http://0.0.0.0:8000\")\n",
    "    mcp.run(transport=\"streamable-http\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Local Test Client\n",
    "\n",
    "The `mcp-client.py` creates a simple test client that:\n",
    "\n",
    "- Connects to the local MCP server at `http://0.0.0.0:8000/mcp`\n",
    "- Lists all available tools\n",
    "- Provides basic connectivity testing\n",
    "\n",
    "This client helps verify that your MCP server is running correctly before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp-client.py\n",
    "#!/usr/bin/env python3\n",
    "import asyncio\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "async def test_server():\n",
    "    mcp_url = \"http://0.0.0.0:8000/mcp\"\n",
    "    \n",
    "    try:\n",
    "        async with streamablehttp_client(mcp_url, {}, terminate_on_close=False) as (\n",
    "            read_stream, write_stream, _\n",
    "        ):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                \n",
    "                tool_result = await session.list_tools()\n",
    "                print(f\"Found {len(tool_result.tools)} tools:\")\n",
    "                \n",
    "                for tool in tool_result.tools:\n",
    "                    print(f\"  - {tool.name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(test_server())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Local Testing Instructions\n",
    "\n",
    "To test your AWS S3 Tables MCP server locally:\n",
    "\n",
    "1. **Terminal 1**: Start the MCP server\n",
    "   ```bash\n",
    "   python mcp-server.py\n",
    "   ```\n",
    "   Expected output: `Starting MCP server in HTTP mode on http://0.0.0.0:8000`\n",
    "   \n",
    "2. **Terminal 2**: Run the test client\n",
    "   ```bash\n",
    "   python mcp-client.py\n",
    "   ```\n",
    "   Expected output: `Found 16 tools:` followed by the list of S3 Tables tools\n",
    "\n",
    "**Note**: Local testing requires AWS credentials to be configured with access to AWS S3 Tables, as the tools make actual calls to AWS S3 Tables services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Amazon Cognito Authentication Setup\n",
    "\n",
    "AgentCore Runtime requires JWT-based authentication. We'll use Amazon Cognito to provide bearer tokens for accessing our deployed MCP server.\n",
    "\n",
    "The `utils.py` file contains helper functions:\n",
    "- `get_cognito_pool_info()`: Retrieves configuration from an existing Cognito User Pool\n",
    "- `setup_cognito_user_pool()`: Creates a new User Pool if needed\n",
    "- `create_agentcore_role()`: Creates the necessary IAM role with proper permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_cognito_pool_info, create_agentcore_role\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pool_id = os.getenv('COGNITO_POOL_ID', 'us-east-1_MYwiFq4Om')\n",
    "region = os.getenv('COGNITO_REGION', 'us-east-1')\n",
    "    \n",
    "print(f\"Get Cognito user pool info for pool id: {pool_id} in region: {region}\")\n",
    "\n",
    "print(\"Setting up Amazon Cognito user pool...\")\n",
    "cognito_config = get_cognito_pool_info(pool_id, region)\n",
    "print(\"Cognito setup completed ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IAM Role Creation\n",
    "\n",
    "Create an IAM execution role for the AgentCore Runtime with:\n",
    "\n",
    "### Base Permissions\n",
    "- Amazon Bedrock model invocation\n",
    "- Amazon ECR image access\n",
    "- CloudWatch logging\n",
    "- X-Ray tracing\n",
    "- AgentCore workload identity access\n",
    "\n",
    "### Additional Permissions\n",
    "- `AmazonS3FullAccess` - Required for S3 operations and data storage\n",
    "- Custom S3 Tables policy - Required for all S3 Tables operations\n",
    "\n",
    "The role is automatically configured with the proper trust policy for bedrock-agentcore.amazonaws.com service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name = \"s3_tables_mcp_server\"\n",
    "additional_managed_policies = ['AmazonS3FullAccess']\n",
    "print(f\"Creating IAM role for {tool_name}...\")\n",
    "agentcore_iam_role = create_agentcore_role(\n",
    "    agent_name=tool_name, \n",
    "    managed_policies=additional_managed_policies\n",
    ")\n",
    "print(f\"IAM role created ‚úì\")\n",
    "print(f\"Role ARN: {agentcore_iam_role['Role']['Arn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AgentCore Runtime Configuration\n",
    "\n",
    "Configure the AgentCore Runtime deployment using the Bedrock AgentCore Starter Toolkit:\n",
    "\n",
    "### Configuration Parameters\n",
    "- **Entrypoint**: `mcp-server.py` (our FastMCP wrapper)\n",
    "- **Execution Role**: The IAM role created above\n",
    "- **Requirements**: `s3-table-requirements.txt` with all dependencies\n",
    "- **Protocol**: MCP (Model Context Protocol)\n",
    "- **Authentication**: Custom JWT authorizer with Cognito\n",
    "\n",
    "### Auto-Generated Resources\n",
    "- Dockerfile optimized for the MCP server\n",
    "- Amazon ECR repository for container storage\n",
    "- AgentCore Runtime configuration\n",
    "\n",
    "The configuration validates that all required files exist before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from boto3.session import Session\n",
    "import time\n",
    "\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(f\"Using AWS region: {region}\")\n",
    "\n",
    "required_files = ['mcp-server.py', 'requirements.txt']\n",
    "for file in required_files:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"Required file {file} not found\")\n",
    "print(\"All required files found ‚úì\")\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "auth_config = {\n",
    "    \"customJWTAuthorizer\": {\n",
    "        \"allowedClients\": [\n",
    "            cognito_config['client_id']\n",
    "        ],\n",
    "        \"discoveryUrl\": cognito_config['discovery_url'],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuring AgentCore Runtime...\")\n",
    "response = agentcore_runtime.configure(\n",
    "    entrypoint=\"mcp-server.py\",\n",
    "    execution_role=agentcore_iam_role['Role']['Arn'],\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=\"s3-table-requirements.txt\",\n",
    "    region=region,\n",
    "    authorizer_configuration=auth_config,\n",
    "    protocol=\"MCP\",\n",
    "    agent_name=tool_name\n",
    ")\n",
    "print(\"Configuration completed ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment to AgentCore Runtime\n",
    "\n",
    "Launch the MCP server to AgentCore Runtime. This process:\n",
    "\n",
    "### Build and Deploy Steps\n",
    "1. **Container Build**: Creates Docker image from the generated Dockerfile\n",
    "2. **ECR Push**: Uploads the container to Amazon ECR\n",
    "3. **Runtime Creation**: Deploys the AgentCore Runtime\n",
    "4. **Service Registration**: Registers the MCP server endpoint\n",
    "\n",
    "### Expected Outputs\n",
    "- Agent ARN: Unique identifier for the deployed runtime\n",
    "- Agent ID: Short identifier for management operations\n",
    "- ECR URI: Container image location\n",
    "\n",
    "**Note**: This process typically takes 5-10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Launching MCP server to AgentCore Runtime...\")\n",
    "print(\"This may take several minutes...\")\n",
    "launch_result = agentcore_runtime.launch()\n",
    "print(\"Launch completed ‚úì\")\n",
    "print(f\"Agent ARN: {launch_result.agent_arn}\")\n",
    "print(f\"Agent ID: {launch_result.agent_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Runtime Status Monitoring\n",
    "\n",
    "Monitor the AgentCore Runtime deployment status:\n",
    "\n",
    "### Status States\n",
    "- **CREATING**: Runtime is being deployed\n",
    "- **READY**: Runtime is operational and ready to serve requests\n",
    "- **CREATE_FAILED**: Deployment failed\n",
    "- **UPDATE_FAILED**: Update operation failed\n",
    "- **DELETE_FAILED**: Deletion operation failed\n",
    "\n",
    "The monitoring loop checks status every 10 seconds until reaching a terminal state. Only proceed to testing when status is **READY**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "print(f\"Initial status: {status}\")\n",
    "\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "while status not in end_status:\n",
    "    print(f\"Status: {status} - waiting...\")\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "\n",
    "if status == 'READY':\n",
    "    print(\"‚úì AgentCore Runtime is READY!\")\n",
    "else:\n",
    "    print(f\"‚ö† AgentCore Runtime status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Storage\n",
    "\n",
    "Store deployment configuration for remote access:\n",
    "\n",
    "### AWS Systems Manager Parameter Store\n",
    "- **Parameter**: `/s3_tables_mcp_server/runtime/agent_arn`\n",
    "- **Value**: The Agent ARN from deployment\n",
    "- **Purpose**: Runtime endpoint identification\n",
    "\n",
    "### AWS Secrets Manager\n",
    "- **Secret**: `s3_tables_mcp_server/cognito/credentials`\n",
    "- **Content**: Complete Cognito configuration including bearer token\n",
    "- **Purpose**: Authentication for remote client access\n",
    "\n",
    "This configuration enables the remote client to authenticate and connect to the deployed MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "ssm_client = boto3.client('ssm', region_name=region)\n",
    "secrets_client = boto3.client('secretsmanager', region_name=region)\n",
    "\n",
    "try:\n",
    "    cognito_credentials_response = secrets_client.create_secret(\n",
    "        Name='s3_tables_mcp_server/cognito/credentials',\n",
    "        Description='Cognito credentials for S3 Tables MCP server',\n",
    "        SecretString=json.dumps(cognito_config)\n",
    "    )\n",
    "    print(\"‚úì Cognito credentials stored in Secrets Manager\")\n",
    "except secrets_client.exceptions.ResourceExistsException:\n",
    "    secrets_client.update_secret(\n",
    "        SecretId='s3_tables_mcp_server/cognito/credentials',\n",
    "        SecretString=json.dumps(cognito_config)\n",
    "    )\n",
    "    print(\"‚úì Cognito credentials updated in Secrets Manager\")\n",
    "\n",
    "agent_arn_response = ssm_client.put_parameter(\n",
    "    Name='/s3_tables_mcp_server/runtime/agent_arn',\n",
    "    Value=launch_result.agent_arn,\n",
    "    Type='String',\n",
    "    Description='Agent ARN for S3 Tables MCP server',\n",
    "    Overwrite=True\n",
    ")\n",
    "print(\"‚úì Agent ARN stored in Parameter Store\")\n",
    "\n",
    "print(\"\\nConfiguration stored successfully!\")\n",
    "print(f\"Agent ARN: {launch_result.agent_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_client_remote.py\n",
    "import asyncio\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "from boto3.session import Session\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "async def main():\n",
    "    boto_session = Session()\n",
    "    region = boto_session.region_name\n",
    "    \n",
    "    print(f\"Using AWS region: {region}\")\n",
    "    \n",
    "    try:\n",
    "        ssm_client = boto3.client('ssm', region_name=region)\n",
    "        agent_arn_response = ssm_client.get_parameter(Name='/s3_tables_mcp_server/runtime/agent_arn')\n",
    "        agent_arn = agent_arn_response['Parameter']['Value']\n",
    "        print(f\"Retrieved Agent ARN: {agent_arn}\")\n",
    "\n",
    "        secrets_client = boto3.client('secretsmanager', region_name=region)\n",
    "        response = secrets_client.get_secret_value(SecretId='s3_tables_mcp_server/cognito/credentials')\n",
    "        secret_value = response['SecretString']\n",
    "        parsed_secret = json.loads(secret_value)\n",
    "        bearer_token = parsed_secret['bearer_token']\n",
    "        print(\"‚úì Retrieved bearer token from Secrets Manager\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving credentials: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if not agent_arn or not bearer_token:\n",
    "        print(\"Error: AGENT_ARN or BEARER_TOKEN not retrieved properly\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    encoded_arn = agent_arn.replace(':', '%3A').replace('/', '%2F')\n",
    "    mcp_url = f\"https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{encoded_arn}/invocations?qualifier=DEFAULT\"\n",
    "    headers = {\n",
    "        \"authorization\": f\"Bearer {bearer_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json, text/event-stream\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nConnecting to: {mcp_url}\")\n",
    "\n",
    "    try:\n",
    "        async with streamablehttp_client(mcp_url, headers, terminate_on_close=False) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                print(\"\\nüîÑ Initializing MCP session...\")\n",
    "                await session.initialize()\n",
    "                \n",
    "                tool_result = await session.list_tools()\n",
    "                \n",
    "                print(\"\\nüìã Available MCP Tools:\")\n",
    "                print(\"=\" * 50)\n",
    "                for tool in tool_result.tools:\n",
    "                    print(f\"üóÇÔ∏è  {tool.name}\")\n",
    "                    print(f\"   Description: {tool.description}\")\n",
    "                    if hasattr(tool, 'inputSchema') and tool.inputSchema:\n",
    "                        properties = tool.inputSchema.get('properties', {})\n",
    "                        if properties:\n",
    "                            print(f\"   Parameters: {list(properties.keys())}\")\n",
    "                    print()\n",
    "                \n",
    "                print(f\"‚úÖ Successfully connected to MCP server!\")\n",
    "                print(f\"Found {len(tool_result.tools)} S3 Tables tools available.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting to MCP server: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Remote Testing\n",
    "\n",
    "Test the deployed MCP server using the remote client (`mcp_client_remote.py`):\n",
    "\n",
    "### Remote Client Features\n",
    "- **Credential Retrieval**: Automatically fetches Agent ARN and bearer token from AWS\n",
    "- **HTTPS Connection**: Connects to the AgentCore Runtime endpoint\n",
    "- **Tool Discovery**: Lists all available S3 Tables tools\n",
    "- **Error Handling**: Provides detailed error messages for troubleshooting\n",
    "\n",
    "### Expected Output\n",
    "The client should display all 16 AWS S3 Tables tools with their descriptions and parameters:\n",
    "- Table bucket management tools (3)\n",
    "- Namespace management tools (2)\n",
    "- Table management tools (5)\n",
    "- Maintenance and monitoring tools (2)\n",
    "- Data operations tools (4)\n",
    "\n",
    "### Connection Details\n",
    "- **Endpoint**: `https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{encoded_arn}/invocations`\n",
    "- **Authentication**: Bearer token from Cognito\n",
    "- **Protocol**: MCP over HTTPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing deployed MCP server...\")\n",
    "print(\"=\" * 50)\n",
    "!python mcp_client_remote.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resource Cleanup\n",
    "\n",
    "Clean up all AWS resources created during this demonstration:\n",
    "\n",
    "### Cleanup Operations\n",
    "1. **Local Configuration**: Remove `.bedrock_agentcore.yaml`\n",
    "2. **AgentCore Runtime**: Delete the deployed runtime\n",
    "3. **ECR Repository**: Remove container images and repository\n",
    "4. **IAM Role**: Detach policies and delete execution role\n",
    "5. **Parameter Store**: Remove stored Agent ARN\n",
    "6. **Secrets Manager**: Delete Cognito credentials\n",
    "\n",
    "### Important Notes\n",
    "- Cleanup is irreversible - ensure you no longer need the deployed resources\n",
    "- Some resources may have dependencies that prevent immediate deletion\n",
    "- Manual cleanup may be required if automated cleanup fails\n",
    "\n",
    "**Warning**: This will permanently delete your deployed MCP server and all associated resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "print(\"üóëÔ∏è  Starting cleanup process...\")\n",
    "\n",
    "agentcore_control_client = boto3.client('bedrock-agentcore-control', region_name=region)\n",
    "ecr_client = boto3.client('ecr', region_name=region)\n",
    "iam_client = boto3.client('iam')\n",
    "ssm_client = boto3.client('ssm', region_name=region)\n",
    "secrets_client = boto3.client('secretsmanager', region_name=region)\n",
    "\n",
    "try:\n",
    "    print(\"Clean up agentcore-config file\")\n",
    "    config_file = \".bedrock_agentcore.yaml\"\n",
    "    if os.path.exists(config_file):\n",
    "        os.remove(config_file)\n",
    "        print(f\"‚úÖ Successfully removed {config_file}\")\n",
    "\n",
    "    print(\"Deleting AgentCore Runtime...\")\n",
    "    runtime_delete_response = agentcore_control_client.delete_agent_runtime(\n",
    "        agentRuntimeId=launch_result.agent_id,\n",
    "    )\n",
    "    print(\"‚úì AgentCore Runtime deletion initiated\")\n",
    "\n",
    "    print(\"Deleting ECR repository...\")\n",
    "    ecr_repo_name = launch_result.ecr_uri.split('/')[1]\n",
    "    ecr_client.delete_repository(\n",
    "        repositoryName=ecr_repo_name,\n",
    "        force=True\n",
    "    )\n",
    "    print(\"‚úì ECR repository deleted\")\n",
    "\n",
    "    print(\"Deleting IAM role policies...\")\n",
    "    policies = iam_client.list_role_policies(\n",
    "        RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "        MaxItems=100\n",
    "    )\n",
    "\n",
    "    for policy_name in policies['PolicyNames']:\n",
    "        iam_client.delete_role_policy(\n",
    "            RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "            PolicyName=policy_name\n",
    "        )\n",
    "    \n",
    "    # List attached managed policies\n",
    "    policies = iam_client.list_attached_role_policies(\n",
    "        RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "        MaxItems=100\n",
    "        )\n",
    "\n",
    "    for policy in policies['AttachedPolicies']:\n",
    "        policy_arn = policy['PolicyArn']\n",
    "        iam_client.detach_role_policy(\n",
    "            RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "            PolicyArn=policy_arn\n",
    "        )\n",
    "\n",
    "    iam_client.delete_role(\n",
    "        RoleName=agentcore_iam_role['Role']['RoleName']\n",
    "    )\n",
    "    print(\"‚úì IAM role deleted\")\n",
    "\n",
    "    try:\n",
    "        ssm_client.delete_parameter(Name='/s3_tables_mcp_server/runtime/agent_arn')\n",
    "        print(\"‚úì Parameter Store parameter deleted\")\n",
    "    except ssm_client.exceptions.ParameterNotFound:\n",
    "        print(\"‚ÑπÔ∏è  Parameter Store parameter not found\")\n",
    "\n",
    "    try:\n",
    "        secrets_client.delete_secret(\n",
    "            SecretId='s3_tables_mcp_server/cognito/credentials',\n",
    "            ForceDeleteWithoutRecovery=True\n",
    "        )\n",
    "        print(\"‚úì Secrets Manager secret deleted\")\n",
    "    except secrets_client.exceptions.ResourceNotFoundException:\n",
    "        print(\"‚ÑπÔ∏è  Secrets Manager secret not found\")\n",
    "\n",
    "    print(\"\\n‚úÖ Cleanup completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during cleanup: {e}\")\n",
    "    print(\"You may need to manually clean up some resources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
