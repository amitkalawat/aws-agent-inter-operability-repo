{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Data Processing MCP Server on AgentCore Runtime\n",
    "\n",
    "This notebook demonstrates how to test and deploy the AWS Data Processing MCP server to Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "## Prerequisites Setup\n",
    "\n",
    "**Step 1:** Clone the AWS MCP repository\n",
    "```bash\n",
    "git clone https://github.com/awslabs/mcp.git\n",
    "```\n",
    "\n",
    "**Step 2:** Copy the AWS Data Processing MCP server to your project root\n",
    "```bash\n",
    "cp -r ./mcp/src/aws-dataprocessing-mcp-server ./\n",
    "```\n",
    "\n",
    "**Step 3:** Set up your environment variables in `.env` file:\n",
    "```\n",
    "COGNITO_POOL_ID=your_pool_id\n",
    "COGNITO_REGION=us-east-1\n",
    "COGNITO_USERNAME=admin\n",
    "COGNITO_CLIENT_SECRET=your_client_secret\n",
    "COGNITO_PASSWORD=your_password\n",
    "AWS_PROFILE=default\n",
    "AWS_REGION=us-east-1\n",
    "CUSTOM_TAGS=false\n",
    "```\n",
    "\n",
    "**Step 4:** Follow the instructions below to complete the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Data Processing MCP Server Overview\n",
    "\n",
    "The AWS Data Processing MCP server provides comprehensive data processing tools and real-time pipeline visibility across AWS Glue, Amazon EMR-EC2, and Amazon Athena. This integration equips AI assistants with 40+ specialized tools organized into 14 handler classes:\n",
    "\n",
    "### AWS Glue Integration (20+ tools)\n",
    "\n",
    "#### Data Catalog Management\n",
    "- **`manage_aws_glue_databases`**: Create, update, delete, and list Glue databases\n",
    "- **`manage_aws_glue_tables`**: Manage tables with schema definition and partitioning\n",
    "- **`manage_aws_glue_connections`**: Configure connections to external data sources\n",
    "- **`manage_aws_glue_partitions`**: Handle table partitions for optimized querying\n",
    "- **`manage_aws_glue_catalog`**: Import and manage external catalogs\n",
    "\n",
    "#### ETL Job Orchestration\n",
    "- **`manage_aws_glue_jobs`**: Create, run, monitor, and manage Glue ETL jobs\n",
    "- **`manage_aws_glue_crawlers`**: Automated data discovery and cataloging\n",
    "- **`manage_aws_glue_classifiers`**: Custom data format detection\n",
    "\n",
    "#### Interactive Development\n",
    "- **`manage_aws_glue_sessions`**: Interactive Spark and Ray workloads\n",
    "- **`manage_aws_glue_statements`**: Execute code in interactive sessions\n",
    "\n",
    "#### Workflow Management\n",
    "- **`manage_aws_glue_workflows`**: Orchestrate complex ETL activities\n",
    "- **`manage_aws_glue_triggers`**: Schedule and automate workflow execution\n",
    "\n",
    "#### Security and Configuration\n",
    "- **`manage_aws_glue_usage_profiles`**: Resource allocation and cost management\n",
    "- **`manage_aws_glue_security_configurations`**: Data encryption settings\n",
    "- **`manage_aws_glue_encryption`**: Catalog encryption management\n",
    "- **`manage_aws_glue_resource_policies`**: Access control policies\n",
    "\n",
    "### Amazon EMR Integration (10+ tools)\n",
    "\n",
    "#### Cluster Management\n",
    "- **`manage_aws_emr_clusters`**: Create, configure, monitor, and terminate EMR clusters\n",
    "- **`manage_aws_emr_ec2_instances`**: Manage instance fleets and groups with auto-scaling\n",
    "- **`manage_aws_emr_ec2_steps`**: Submit and monitor Hadoop, Spark, and other job steps\n",
    "\n",
    "### Amazon Athena Integration (10+ tools)\n",
    "\n",
    "#### Query Management\n",
    "- **`manage_aws_athena_query_executions`**: Execute, monitor, and manage SQL queries\n",
    "- **`manage_aws_athena_named_queries`**: Create reusable query libraries\n",
    "\n",
    "#### Data Catalog Operations\n",
    "- **`manage_aws_athena_data_catalogs`**: Manage multiple catalog types (LAMBDA, GLUE, HIVE, FEDERATED)\n",
    "- **`manage_aws_athena_databases_and_tables`**: Database and table metadata discovery\n",
    "- **`manage_aws_athena_workgroups`**: Cost control and access management\n",
    "\n",
    "### Common Resource Management\n",
    "\n",
    "#### IAM and S3 Tools\n",
    "- **`add_inline_policy`**: Create custom IAM policies for data processing services\n",
    "- **`get_policies_for_role`**: Retrieve role permissions\n",
    "- **`create_data_processing_role`**: Create specialized IAM roles\n",
    "- **`get_roles_for_service`**: Find service-specific roles\n",
    "- **`list_s3_buckets`**: Analyze S3 bucket usage for data processing\n",
    "- **`upload_to_s3`**: Upload scripts and code to S3\n",
    "- **`analyze_s3_usage_for_data_processing`**: Usage pattern analysis\n",
    "\n",
    "### Key Features\n",
    "- **Read/Write Modes**: Optional `--allow-write` flag for mutating operations\n",
    "- **Sensitive Data Access**: `--allow-sensitive-data-access` for logs and events\n",
    "- **Resource Tagging**: MCP-managed resource tracking with optional `CUSTOM_TAGS` override\n",
    "- **Multi-Service Integration**: Seamless workflows across Glue, EMR, and Athena\n",
    "- **Natural Language Interface**: Complex data operations through conversational AI\n",
    "\n",
    "All tools require appropriate AWS permissions and work together to provide end-to-end data processing pipeline management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "### System Requirements\n",
    "- Python 3.10 or higher\n",
    "- AWS CLI configured with valid credentials\n",
    "- Docker installed (for containerization)\n",
    "\n",
    "### AWS Permissions\n",
    "Your AWS credentials must have comprehensive permissions for:\n",
    "\n",
    "#### Required for All Operations\n",
    "- Amazon Bedrock AgentCore\n",
    "- Amazon ECR (for container registry)\n",
    "- Amazon Cognito (for authentication)\n",
    "- IAM (for role creation)\n",
    "- AWS Systems Manager Parameter Store\n",
    "- AWS Secrets Manager\n",
    "\n",
    "#### Data Processing Services\n",
    "- **AWS Glue**: Full access for data catalog, ETL jobs, crawlers, interactive sessions\n",
    "- **Amazon EMR**: Cluster management, instance operations, step execution\n",
    "- **Amazon Athena**: Query execution, workgroup management, data catalog operations\n",
    "- **Amazon S3**: Data storage and script hosting\n",
    "- **CloudWatch**: Logging and monitoring\n",
    "\n",
    "**Security Note**: This MCP server requires extensive permissions for comprehensive data processing operations. Use appropriate access controls in production environments.\n",
    "\n",
    "### Project Structure\n",
    "- `aws-dataprocessing-mcp-server/` - The MCP server implementation\n",
    "- `dataprocessing-requirements.txt` - Python dependencies for data processing\n",
    "- `utils.py` - Helper functions for Cognito and IAM setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install all required Python packages using uv (recommended) or pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS Profile\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = 'ibc2025'\n",
    "print(f\"AWS Profile set to: {os.environ['AWS_PROFILE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current Python and install packages directly\n",
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Install packages using the current Python interpreter\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"dataprocessing-requirements.txt\"])\n",
    "    print(\"‚úì All packages installed successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error installing packages: {e}\")\n",
    "    \n",
    "# Verify key packages are available\n",
    "try:\n",
    "    import boto3\n",
    "    print(\"‚úì boto3 available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå boto3 not available\")\n",
    "    \n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"‚úì python-dotenv available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå python-dotenv not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS Profile\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = 'ibc2025'\n",
    "print(f\"AWS Profile set to: {os.environ['AWS_PROFILE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r dataprocessing-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local Testing\n",
    "\n",
    "Before deploying to AgentCore Runtime, let's test the AWS Data Processing MCP server locally.\n",
    "\n",
    "### 2.1 MCP Server Wrapper\n",
    "\n",
    "The `mcp-server.py` file creates a FastMCP wrapper around the AWS Data Processing MCP server for AgentCore deployment. This wrapper:\n",
    "\n",
    "- Imports all 14 handler classes from the original server implementation\n",
    "- Registers 40+ tools across Glue, EMR, and Athena services\n",
    "- Configures the server for HTTP transport on port 8000\n",
    "- Supports `--allow-write` and `--allow-sensitive-data-access` flags\n",
    "- Enables stateless HTTP mode for AgentCore compatibility\n",
    "- Provides comprehensive instructions for data processing workflows\n",
    "\n",
    "The server exposes tools from these handler classes:\n",
    "- **Glue Handlers**: Data Catalog, ETL Jobs, Crawlers, Interactive Sessions, Workflows, Commons\n",
    "- **EMR Handlers**: Clusters, Instances, Steps\n",
    "- **Athena Handlers**: Queries, Data Catalogs, Workgroups\n",
    "- **Common Handlers**: IAM and S3 resource management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp-server.py\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# Add the path before any imports\n",
    "sys.path.insert(0, os.path.abspath(\"./aws-dataprocessing-mcp-server\"))\n",
    "\n",
    "# Import all handler classes from the original server\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.athena.athena_data_catalog_handler import AthenaDataCatalogHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.athena.athena_query_handler import AthenaQueryHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.athena.athena_workgroup_handler import AthenaWorkGroupHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.commons.common_resource_handler import CommonResourceHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.emr.emr_ec2_cluster_handler import EMREc2ClusterHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.emr.emr_ec2_instance_handler import EMREc2InstanceHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.emr.emr_ec2_steps_handler import EMREc2StepsHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.glue.crawler_handler import CrawlerHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.glue.data_catalog_handler import GlueDataCatalogHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.glue.glue_commons_handler import GlueCommonsHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.glue.glue_etl_handler import GlueEtlJobsHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.glue.interactive_sessions_handler import GlueInteractiveSessionsHandler\n",
    "from awslabs.aws_dataprocessing_mcp_server.handlers.glue.worklows_handler import GlueWorkflowAndTriggerHandler\n",
    "\n",
    "# Create a new FastMCP instance with correct parameters\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Parse command line arguments\n",
    "parser = argparse.ArgumentParser(description='AWS Data Processing MCP Server')\n",
    "parser.add_argument('--allow-write', action='store_true', help='Enable write operations')\n",
    "parser.add_argument('--allow-sensitive-data-access', action='store_true', help='Allow access to sensitive data')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Store flags in environment variables for handlers\n",
    "if args.allow_write:\n",
    "    os.environ['ALLOW_WRITE'] = 'true'\n",
    "if args.allow_sensitive_data_access:\n",
    "    os.environ['ALLOW_SENSITIVE_DATA_ACCESS'] = 'true'\n",
    "\n",
    "mcp = FastMCP(\n",
    "    'awslabs.aws-dataprocessing-mcp-server',\n",
    "    host=\"0.0.0.0\",\n",
    "    stateless_http=True,\n",
    "    instructions=\"\"\"AWS Data Processing MCP Server provides comprehensive tools for managing AWS data processing services including Glue, EMR, and Athena.\n",
    "\n",
    "    This server enables you to:\n",
    "    - Manage AWS Glue Data Catalog with databases, tables, connections, and partitions\n",
    "    - Create and orchestrate ETL jobs with automated crawlers and interactive sessions\n",
    "    - Deploy and manage Amazon EMR clusters with instance fleet management\n",
    "    - Execute and monitor Hadoop, Spark, and other big data processing steps\n",
    "    - Run SQL queries through Amazon Athena with workgroup management\n",
    "    - Configure security, encryption, and access policies across all services\n",
    "    - Manage IAM roles and S3 resources for data processing workflows\n",
    "\n",
    "    ## Available Tool Categories:\n",
    "    \n",
    "    ### AWS Glue Tools (20+ tools)\n",
    "    - **Data Catalog**: manage_aws_glue_databases, manage_aws_glue_tables, manage_aws_glue_connections, manage_aws_glue_partitions, manage_aws_glue_catalog\n",
    "    - **ETL Jobs**: manage_aws_glue_jobs (create, run, monitor job runs, bookmarks)\n",
    "    - **Crawlers**: manage_aws_glue_crawlers, manage_aws_glue_classifiers\n",
    "    - **Interactive Sessions**: manage_aws_glue_sessions, manage_aws_glue_statements\n",
    "    - **Workflows**: manage_aws_glue_workflows, manage_aws_glue_triggers\n",
    "    - **Security & Config**: manage_aws_glue_usage_profiles, manage_aws_glue_security_configurations, manage_aws_glue_encryption, manage_aws_glue_resource_policies\n",
    "    \n",
    "    ### Amazon EMR Tools (10+ tools)\n",
    "    - **Cluster Management**: manage_aws_emr_clusters (create, configure, terminate clusters)\n",
    "    - **Instance Management**: manage_aws_emr_ec2_instances (fleet and group management)\n",
    "    - **Step Execution**: manage_aws_emr_ec2_steps (submit and monitor jobs)\n",
    "    \n",
    "    ### Amazon Athena Tools (10+ tools)\n",
    "    - **Query Execution**: manage_aws_athena_query_executions (execute, monitor SQL queries)\n",
    "    - **Named Queries**: manage_aws_athena_named_queries (reusable query libraries)\n",
    "    - **Data Catalogs**: manage_aws_athena_data_catalogs (LAMBDA, GLUE, HIVE, FEDERATED)\n",
    "    - **Discovery**: manage_aws_athena_databases_and_tables (metadata exploration)\n",
    "    - **Workgroups**: manage_aws_athena_workgroups (cost control, access management)\n",
    "    \n",
    "    ### Common Resource Tools\n",
    "    - **IAM Management**: add_inline_policy, get_policies_for_role, create_data_processing_role, get_roles_for_service\n",
    "    - **S3 Operations**: list_s3_buckets, upload_to_s3, analyze_s3_usage_for_data_processing\n",
    "\n",
    "    ## Operation Modes:\n",
    "    - **Read-Only Mode** (default): Safe exploration and monitoring operations\n",
    "    - **Write Mode** (--allow-write): Enable resource creation, modification, and deletion\n",
    "    - **Sensitive Data Access** (--allow-sensitive-data-access): Access logs, events, and sensitive configurations\n",
    "\n",
    "    ## Common Workflows:\n",
    "    1. **Data Discovery**: Create crawlers ‚Üí Generate tables ‚Üí Query with Athena\n",
    "    2. **ETL Pipeline**: Design Glue jobs ‚Üí Configure workflows ‚Üí Monitor execution\n",
    "    3. **Big Data Processing**: Launch EMR clusters ‚Üí Submit steps ‚Üí Process at scale\n",
    "    4. **Analytics**: Set up Athena workgroups ‚Üí Execute queries ‚Üí Analyze results\n",
    "\n",
    "    ## Resource Management:\n",
    "    - All resources are tagged for MCP management (unless CUSTOM_TAGS=true)\n",
    "    - Only MCP-created resources can be modified or deleted through this server\n",
    "    - IAM roles and policies are automatically configured for service access\n",
    "\n",
    "    For more information about AWS Data Processing services, visit:\n",
    "    - AWS Glue: https://aws.amazon.com/glue/\n",
    "    - Amazon EMR: https://aws.amazon.com/emr/\n",
    "    - Amazon Athena: https://aws.amazon.com/athena/\n",
    "    \"\"\",\n",
    "    dependencies=[\n",
    "        'pydantic',\n",
    "        'loguru',\n",
    "        'boto3',\n",
    "        'requests',\n",
    "        'pyyaml',\n",
    "        'cachetools',\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Initialize handlers with write permissions\n",
    "allow_write = args.allow_write\n",
    "allow_sensitive = args.allow_sensitive_data_access\n",
    "\n",
    "# Athena handlers\n",
    "athena_data_catalog_handler = AthenaDataCatalogHandler(allow_write=allow_write)\n",
    "athena_query_handler = AthenaQueryHandler(allow_write=allow_write, allow_sensitive_data_access=allow_sensitive)\n",
    "athena_workgroup_handler = AthenaWorkGroupHandler(allow_write=allow_write)\n",
    "\n",
    "# Common resource handler\n",
    "common_resource_handler = CommonResourceHandler(allow_write=allow_write)\n",
    "\n",
    "# EMR handlers\n",
    "emr_cluster_handler = EMREc2ClusterHandler(allow_write=allow_write)\n",
    "emr_instance_handler = EMREc2InstanceHandler(allow_write=allow_write)\n",
    "emr_steps_handler = EMREc2StepsHandler(allow_write=allow_write)\n",
    "\n",
    "# Glue handlers\n",
    "crawler_handler = CrawlerHandler(allow_write=allow_write)\n",
    "glue_data_catalog_handler = GlueDataCatalogHandler(allow_write=allow_write)\n",
    "glue_commons_handler = GlueCommonsHandler(allow_write=allow_write)\n",
    "glue_etl_handler = GlueEtlJobsHandler(allow_write=allow_write)\n",
    "glue_sessions_handler = GlueInteractiveSessionsHandler(allow_write=allow_write, allow_sensitive_data_access=allow_sensitive)\n",
    "glue_workflow_handler = GlueWorkflowAndTriggerHandler(allow_write=allow_write)\n",
    "\n",
    "# Register all tools from handlers\n",
    "# Athena tools\n",
    "mcp.tool(name='manage_aws_athena_data_catalogs')(athena_data_catalog_handler.manage_aws_athena_data_catalogs)\n",
    "mcp.tool(name='manage_aws_athena_databases_and_tables')(athena_data_catalog_handler.manage_aws_athena_databases_and_tables)\n",
    "mcp.tool(name='manage_aws_athena_query_executions')(athena_query_handler.manage_aws_athena_query_executions)\n",
    "mcp.tool(name='manage_aws_athena_named_queries')(athena_query_handler.manage_aws_athena_named_queries)\n",
    "mcp.tool(name='manage_aws_athena_workgroups')(athena_workgroup_handler.manage_aws_athena_workgroups)\n",
    "\n",
    "# Common resource tools\n",
    "mcp.tool(name='add_inline_policy')(common_resource_handler.add_inline_policy)\n",
    "mcp.tool(name='get_policies_for_role')(common_resource_handler.get_policies_for_role)\n",
    "mcp.tool(name='create_data_processing_role')(common_resource_handler.create_data_processing_role)\n",
    "mcp.tool(name='get_roles_for_service')(common_resource_handler.get_roles_for_service)\n",
    "mcp.tool(name='list_s3_buckets')(common_resource_handler.list_s3_buckets)\n",
    "mcp.tool(name='upload_to_s3')(common_resource_handler.upload_to_s3)\n",
    "mcp.tool(name='analyze_s3_usage_for_data_processing')(common_resource_handler.analyze_s3_usage_for_data_processing)\n",
    "\n",
    "# EMR tools\n",
    "mcp.tool(name='manage_aws_emr_clusters')(emr_cluster_handler.manage_aws_emr_clusters)\n",
    "mcp.tool(name='manage_aws_emr_ec2_instances')(emr_instance_handler.manage_aws_emr_ec2_instances)\n",
    "mcp.tool(name='manage_aws_emr_ec2_steps')(emr_steps_handler.manage_aws_emr_ec2_steps)\n",
    "\n",
    "# Glue tools\n",
    "mcp.tool(name='manage_aws_glue_crawlers')(crawler_handler.manage_aws_glue_crawlers)\n",
    "mcp.tool(name='manage_aws_glue_classifiers')(crawler_handler.manage_aws_glue_classifiers)\n",
    "mcp.tool(name='manage_aws_glue_crawler_management')(crawler_handler.manage_aws_glue_crawler_management)\n",
    "mcp.tool(name='manage_aws_glue_databases')(glue_data_catalog_handler.manage_aws_glue_databases)\n",
    "mcp.tool(name='manage_aws_glue_tables')(glue_data_catalog_handler.manage_aws_glue_tables)\n",
    "mcp.tool(name='manage_aws_glue_connections')(glue_data_catalog_handler.manage_aws_glue_connections)\n",
    "mcp.tool(name='manage_aws_glue_partitions')(glue_data_catalog_handler.manage_aws_glue_partitions)\n",
    "mcp.tool(name='manage_aws_glue_catalog')(glue_data_catalog_handler.manage_aws_glue_catalog)\n",
    "mcp.tool(name='manage_aws_glue_usage_profiles')(glue_commons_handler.manage_aws_glue_usage_profiles)\n",
    "mcp.tool(name='manage_aws_glue_security_configurations')(glue_commons_handler.manage_aws_glue_security_configurations)\n",
    "mcp.tool(name='manage_aws_glue_encryption')(glue_commons_handler.manage_aws_glue_encryption)\n",
    "mcp.tool(name='manage_aws_glue_resource_policies')(glue_commons_handler.manage_aws_glue_resource_policies)\n",
    "mcp.tool(name='manage_aws_glue_jobs')(glue_etl_handler.manage_aws_glue_jobs)\n",
    "mcp.tool(name='manage_aws_glue_sessions')(glue_sessions_handler.manage_aws_glue_sessions)\n",
    "mcp.tool(name='manage_aws_glue_statements')(glue_sessions_handler.manage_aws_glue_statements)\n",
    "mcp.tool(name='manage_aws_glue_workflows')(glue_workflow_handler.manage_aws_glue_workflows)\n",
    "mcp.tool(name='manage_aws_glue_triggers')(glue_workflow_handler.manage_aws_glue_triggers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    write_mode = \"with write access\" if args.allow_write else \"in read-only mode\"\n",
    "    sensitive_mode = \" and sensitive data access\" if args.allow_sensitive_data_access else \"\"\n",
    "    print(f\"Starting AWS Data Processing MCP server {write_mode}{sensitive_mode} on http://0.0.0.0:8000\")\n",
    "    mcp.run(transport=\"streamable-http\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Local Test Client\n",
    "\n",
    "The `mcp-client.py` creates a simple test client that:\n",
    "\n",
    "- Connects to the local MCP server at `http://0.0.0.0:8000/mcp`\n",
    "- Lists all available tools\n",
    "- Provides basic connectivity testing\n",
    "\n",
    "This client helps verify that your MCP server is running correctly before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp-client.py\n",
    "#!/usr/bin/env python3\n",
    "import asyncio\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "async def test_server():\n",
    "    mcp_url = \"http://0.0.0.0:8000/mcp\"\n",
    "    \n",
    "    try:\n",
    "        async with streamablehttp_client(mcp_url, {}, terminate_on_close=False) as (\n",
    "            read_stream, write_stream, _\n",
    "        ):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                \n",
    "                tool_result = await session.list_tools()\n",
    "                print(f\"Found {len(tool_result.tools)} tools:\")\n",
    "                \n",
    "                # Group tools by service\n",
    "                glue_tools = [t.name for t in tool_result.tools if 'glue' in t.name]\n",
    "                emr_tools = [t.name for t in tool_result.tools if 'emr' in t.name]\n",
    "                athena_tools = [t.name for t in tool_result.tools if 'athena' in t.name]\n",
    "                common_tools = [t.name for t in tool_result.tools if t.name not in glue_tools + emr_tools + athena_tools]\n",
    "                \n",
    "                print(f\"\\nAWS Glue tools ({len(glue_tools)}):\")\n",
    "                for tool in glue_tools:\n",
    "                    print(f\"  - {tool}\")\n",
    "                    \n",
    "                print(f\"\\nAmazon EMR tools ({len(emr_tools)}):\")\n",
    "                for tool in emr_tools:\n",
    "                    print(f\"  - {tool}\")\n",
    "                    \n",
    "                print(f\"\\nAmazon Athena tools ({len(athena_tools)}):\")\n",
    "                for tool in athena_tools:\n",
    "                    print(f\"  - {tool}\")\n",
    "                    \n",
    "                print(f\"\\nCommon resource tools ({len(common_tools)}):\")\n",
    "                for tool in common_tools:\n",
    "                    print(f\"  - {tool}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(test_server())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Local Testing Instructions\n",
    "\n",
    "To test your AWS Data Processing MCP server locally:\n",
    "\n",
    "1. **Terminal 1**: Start the MCP server\n",
    "   ```bash\n",
    "   python mcp-server.py --allow-write --allow-sensitive-data-access\n",
    "   ```\n",
    "   Expected output: `Starting AWS Data Processing MCP server with write access and sensitive data access on http://0.0.0.0:8000`\n",
    "   \n",
    "2. **Terminal 2**: Run the test client\n",
    "   ```bash\n",
    "   python mcp-client.py\n",
    "   ```\n",
    "   Expected output: `Found 25+ tools:` followed by categorized tool lists\n",
    "\n",
    "**Note**: Local testing requires AWS credentials with appropriate permissions for Glue, EMR, and Athena services. The server will start but tools may fail without proper AWS access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Amazon Cognito Authentication Setup\n",
    "\n",
    "AgentCore Runtime requires JWT-based authentication. We'll use Amazon Cognito to provide bearer tokens for accessing our deployed MCP server.\n",
    "\n",
    "The `utils.py` file contains helper functions:\n",
    "- `get_cognito_pool_info()`: Retrieves configuration from an existing Cognito User Pool\n",
    "- `setup_cognito_user_pool()`: Creates a new User Pool if needed\n",
    "- `create_agentcore_role()`: Creates the necessary IAM role with proper permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils import get_cognito_pool_info, create_agentcore_role\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pool_id = os.getenv('COGNITO_POOL_ID', 'us-east-1_XXXXX')\n",
    "region = os.getenv('COGNITO_REGION', 'us-east-1')\n",
    "    \n",
    "print(f\"Get Cognito user pool info for pool id: {pool_id} in region: {region}\")\n",
    "\n",
    "print(\"Setting up Amazon Cognito user pool...\")\n",
    "cognito_config = get_cognito_pool_info(pool_id, region)\n",
    "print(\"Cognito setup completed ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tool_name = \"dataprocessing_mcp_server\"  # Fixed: Remove 'aws_' prefix to meet validation rules\n",
    "additional_managed_policies = [\n",
    "    'AWSGlueConsoleFullAccess',  # Fixed: Use console access instead of service role\n",
    "    'AmazonS3FullAccess', \n",
    "    'AmazonEMRFullAccessPolicy_v2',\n",
    "    'AmazonAthenaFullAccess'\n",
    "]\n",
    "print(f\"Creating IAM role for {tool_name}...\")\n",
    "agentcore_iam_role = create_agentcore_role(\n",
    "    agent_name=tool_name, \n",
    "    managed_policies=additional_managed_policies\n",
    ")\n",
    "print(f\"IAM role created ‚úì\")\n",
    "print(f\"Role ARN: {agentcore_iam_role['Role']['Arn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name = \"dataproc_mcp_ibcv2\"\n",
    "additional_managed_policies = [\n",
    "    'AWSGlueConsoleFullAccess',\n",
    "    'AmazonS3FullAccess', \n",
    "    'AmazonEMRFullAccessPolicy_v2',\n",
    "    'AmazonAthenaFullAccess'\n",
    "]\n",
    "print(f\"Creating IAM role for {tool_name}...\")\n",
    "agentcore_iam_role = create_agentcore_role(\n",
    "    agent_name=tool_name, \n",
    "    managed_policies=additional_managed_policies\n",
    ")\n",
    "print(f\"IAM role created ‚úì\")\n",
    "print(f\"Role ARN: {agentcore_iam_role['Role']['Arn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role_name = 'agentcore-dataproc_mcp_ibcv2-role'  # Updated for new tool name\n",
    "\n",
    "# Define the correct trust policy for bedrock-agentcore\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AssumeRolePolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock-agentcore.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock-agentcore:{region}:{account_id}:*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Updating trust policy for role: {role_name}\")\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "try:\n",
    "    # Update the assume role policy\n",
    "    iam_client.update_assume_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "    print(\"‚úÖ Trust policy updated successfully\")\n",
    "    \n",
    "    # Verify the update\n",
    "    role_info = iam_client.get_role(RoleName=role_name)\n",
    "    print(\"\\n‚úÖ Role verification:\")\n",
    "    print(f\"  - Role ARN: {role_info['Role']['Arn']}\")\n",
    "    print(f\"  - Trust policy updated: Yes\")\n",
    "    \n",
    "    # Wait for propagation\n",
    "    import time\n",
    "    print(\"\\n‚è≥ Waiting 30 seconds for IAM changes to propagate...\")\n",
    "    time.sleep(30)\n",
    "    print(\"‚úÖ IAM propagation wait completed\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Role is now ready. You can retry the launch operation.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error updating trust policy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role_name = 'agentcore-dataproc_mcp_ibcv2-role'  # Updated for new tool name\n",
    "\n",
    "# Define the corrected ECR policy\n",
    "ecr_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"ECRAuthToken\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:GetAuthorizationToken\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"ECRImageAccess\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:GetDownloadUrlForLayer\",\n",
    "                \"ecr:BatchCheckLayerAvailability\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:ecr:{region}:{account_id}:repository/bedrock-agentcore-dataprocessing_mcp_server\",\n",
    "                f\"arn:aws:ecr:{region}:{account_id}:repository/bedrock-agentcore-dataprocessing_mcp_server/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Adding ECR permissions to role: {role_name}\")\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "try:\n",
    "    # Add the ECR policy as an inline policy\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName='ECRAccessPolicy',\n",
    "        PolicyDocument=json.dumps(ecr_policy)\n",
    "    )\n",
    "    print(\"‚úÖ ECR permissions added successfully\")\n",
    "    \n",
    "    # List all policies attached to the role\n",
    "    inline_policies = iam_client.list_role_policies(RoleName=role_name)\n",
    "    print(f\"\\nüìã Inline policies attached to role:\")\n",
    "    for policy in inline_policies['PolicyNames']:\n",
    "        print(f\"  - {policy}\")\n",
    "    \n",
    "    # Wait for propagation\n",
    "    import time\n",
    "    print(\"\\n‚è≥ Waiting 20 seconds for IAM changes to propagate...\")\n",
    "    time.sleep(20)\n",
    "    print(\"‚úÖ IAM propagation wait completed\")\n",
    "    \n",
    "    print(\"\\n‚úÖ ECR permissions are now properly configured. You can retry the launch operation.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error adding ECR permissions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AgentCore Runtime Configuration\n",
    "\n",
    "Configure the AgentCore Runtime deployment using the Bedrock AgentCore Starter Toolkit:\n",
    "\n",
    "### Configuration Parameters\n",
    "- **Entrypoint**: `mcp-server.py` (our FastMCP wrapper with handler registration)\n",
    "- **Execution Role**: The IAM role created above with data processing permissions\n",
    "- **Requirements**: `dataprocessing-requirements.txt` with all dependencies\n",
    "- **Protocol**: MCP (Model Context Protocol)\n",
    "- **Authentication**: Custom JWT authorizer with Cognito\n",
    "\n",
    "### Auto-Generated Resources\n",
    "- Dockerfile optimized for the MCP server with data processing dependencies\n",
    "- Amazon ECR repository for container storage\n",
    "- AgentCore Runtime configuration with environment variables\n",
    "\n",
    "The configuration validates that all required files exist before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from boto3.session import Session\n",
    "import time\n",
    "\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(f\"Using AWS region: {region}\")\n",
    "\n",
    "required_files = ['mcp-server.py', 'dataprocessing-requirements.txt']\n",
    "for file in required_files:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"Required file {file} not found\")\n",
    "print(\"All required files found ‚úì\")\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "auth_config = {\n",
    "    \"customJWTAuthorizer\": {\n",
    "        \"allowedClients\": [\n",
    "            cognito_config['client_id']\n",
    "        ],\n",
    "        \"discoveryUrl\": cognito_config['discovery_url'],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuring AgentCore Runtime...\")\n",
    "response = agentcore_runtime.configure(\n",
    "    entrypoint=\"mcp-server.py\",\n",
    "    execution_role=agentcore_iam_role['Role']['Arn'],\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=\"dataprocessing-requirements.txt\",\n",
    "    region=region,\n",
    "    authorizer_configuration=auth_config,\n",
    "    protocol=\"MCP\",\n",
    "    agent_name=tool_name\n",
    ")\n",
    "print(\"Configuration completed ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment to AgentCore Runtime\n",
    "\n",
    "Launch the MCP server to AgentCore Runtime. This process:\n",
    "\n",
    "### Build and Deploy Steps\n",
    "1. **Container Build**: Creates Docker image from the generated Dockerfile with data processing dependencies\n",
    "2. **ECR Push**: Uploads the container to Amazon ECR\n",
    "3. **Runtime Creation**: Deploys the AgentCore Runtime with comprehensive permissions\n",
    "4. **Service Registration**: Registers the MCP server endpoint with 25+ data processing tools\n",
    "\n",
    "### Expected Outputs\n",
    "- Agent ARN: Unique identifier for the deployed runtime\n",
    "- Agent ID: Short identifier for management operations\n",
    "- ECR URI: Container image location\n",
    "\n",
    "**Note**: This process typically takes 8-12 minutes due to the comprehensive dependencies and handler registrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Launching MCP server to AgentCore Runtime...\")\n",
    "print(\"This may take several minutes due to data processing dependencies...\")\n",
    "launch_result = agentcore_runtime.launch()\n",
    "print(\"Launch completed ‚úì\")\n",
    "print(f\"Agent ARN: {launch_result.agent_arn}\")\n",
    "print(f\"Agent ID: {launch_result.agent_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Runtime Status Monitoring\n",
    "\n",
    "Monitor the AgentCore Runtime deployment status:\n",
    "\n",
    "### Status States\n",
    "- **CREATING**: Runtime is being deployed\n",
    "- **READY**: Runtime is operational and ready to serve requests\n",
    "- **CREATE_FAILED**: Deployment failed\n",
    "- **UPDATE_FAILED**: Update operation failed\n",
    "- **DELETE_FAILED**: Deletion operation failed\n",
    "\n",
    "The monitoring loop checks status every 10 seconds until reaching a terminal state. Only proceed to testing when status is **READY**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "print(f\"Initial status: {status}\")\n",
    "\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "while status not in end_status:\n",
    "    print(f\"Status: {status} - waiting...\")\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "\n",
    "if status == 'READY':\n",
    "    print(\"‚úì AgentCore Runtime is READY!\")\n",
    "else:\n",
    "    print(f\"‚ö† AgentCore Runtime status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "ssm_client = boto3.client('ssm', region_name=region)\n",
    "secrets_client = boto3.client('secretsmanager', region_name=region)\n",
    "\n",
    "# Store Cognito credentials in Secrets Manager with the actual path used in the client\n",
    "try:\n",
    "    cognito_credentials_response = secrets_client.create_secret(\n",
    "        Name='mcp/aws_dataprocessing_server-ibc/cognito/credentials',\n",
    "        Description='Cognito credentials for AWS Data Processing MCP server',\n",
    "        SecretString=json.dumps(cognito_config)\n",
    "    )\n",
    "    print(\"‚úì Cognito credentials stored in Secrets Manager\")\n",
    "except secrets_client.exceptions.ResourceExistsException:\n",
    "    secrets_client.update_secret(\n",
    "        SecretId='mcp/aws_dataprocessing_server-ibc/cognito/credentials',\n",
    "        SecretString=json.dumps(cognito_config)\n",
    "    )\n",
    "    print(\"‚úì Cognito credentials updated in Secrets Manager\")\n",
    "\n",
    "# Store the actual agent ARN in Parameter Store\n",
    "agent_arn_response = ssm_client.put_parameter(\n",
    "    Name='/mcp/aws_dataprocessing_server-ibc/runtime/agent_arn',\n",
    "    Value=launch_result.agent_arn,\n",
    "    Type='String',\n",
    "    Description='Agent ARN for AWS Data Processing MCP server',\n",
    "    Overwrite=True\n",
    ")\n",
    "print(\"‚úì Agent ARN stored in Parameter Store\")\n",
    "\n",
    "print(\"\\nConfiguration stored successfully!\")\n",
    "print(f\"Agent ARN: {launch_result.agent_arn}\")\n",
    "print(f\"Parameter path: /mcp/aws_dataprocessing_server-ibc/runtime/agent_arn\")\n",
    "print(f\"Secret path: mcp/aws_dataprocessing_server-ibc/cognito/credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_client_remote.py\n",
    "import asyncio\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "from boto3.session import Session\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "async def main():\n",
    "    boto_session = Session()\n",
    "    region = boto_session.region_name\n",
    "    \n",
    "    print(f\"Using AWS region: {region}\")\n",
    "    \n",
    "    try:\n",
    "        ssm_client = boto3.client('ssm', region_name=region)\n",
    "        agent_arn_response = ssm_client.get_parameter(Name='/mcp/aws_dataprocessing_server-ibc/runtime/agent_arn')\n",
    "        agent_arn = agent_arn_response['Parameter']['Value']\n",
    "        print(f\"Retrieved Agent ARN: {agent_arn}\")\n",
    "\n",
    "        secrets_client = boto3.client('secretsmanager', region_name=region)\n",
    "        response = secrets_client.get_secret_value(SecretId='mcp/aws_dataprocessing_server-ibc/cognito/credentials')\n",
    "        secret_value = response['SecretString']\n",
    "        parsed_secret = json.loads(secret_value)\n",
    "        bearer_token = parsed_secret['bearer_token']\n",
    "        print(\"‚úì Retrieved bearer token from Secrets Manager\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving credentials: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if not agent_arn or not bearer_token:\n",
    "        print(\"Error: AGENT_ARN or BEARER_TOKEN not retrieved properly\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    encoded_arn = agent_arn.replace(':', '%3A').replace('/', '%2F')\n",
    "    mcp_url = f\"https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{encoded_arn}/invocations?qualifier=DEFAULT\"\n",
    "    headers = {\n",
    "        \"authorization\": f\"Bearer {bearer_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json, text/event-stream\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nConnecting to: {mcp_url}\")\n",
    "\n",
    "    try:\n",
    "        async with streamablehttp_client(mcp_url, headers, terminate_on_close=False) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                print(\"\\nüîÑ Initializing MCP session...\")\n",
    "                await session.initialize()\n",
    "                \n",
    "                tool_result = await session.list_tools()\n",
    "                \n",
    "                # Group tools by service\n",
    "                glue_tools = [tool for tool in tool_result.tools if 'glue' in tool.name]\n",
    "                emr_tools = [tool for tool in tool_result.tools if 'emr' in tool.name]\n",
    "                athena_tools = [tool for tool in tool_result.tools if 'athena' in tool.name]\n",
    "                common_tools = [tool for tool in tool_result.tools if tool not in glue_tools + emr_tools + athena_tools]\n",
    "                \n",
    "                print(\"\\nüìã Available MCP Tools:\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                print(f\"\\nüîß AWS Glue Tools ({len(glue_tools)} tools):\")\n",
    "                for tool in glue_tools:\n",
    "                    print(f\"   ‚Ä¢ {tool.name}\")\n",
    "                    \n",
    "                print(f\"\\nüöÄ Amazon EMR Tools ({len(emr_tools)} tools):\")\n",
    "                for tool in emr_tools:\n",
    "                    print(f\"   ‚Ä¢ {tool.name}\")\n",
    "                    \n",
    "                print(f\"\\nüìä Amazon Athena Tools ({len(athena_tools)} tools):\")\n",
    "                for tool in athena_tools:\n",
    "                    print(f\"   ‚Ä¢ {tool.name}\")\n",
    "                    \n",
    "                print(f\"\\nüõ†Ô∏è  Common Resource Tools ({len(common_tools)} tools):\")\n",
    "                for tool in common_tools:\n",
    "                    print(f\"   ‚Ä¢ {tool.name}\")\n",
    "                \n",
    "                print(f\"\\n‚úÖ Successfully connected to MCP server!\")\n",
    "                print(f\"Found {len(tool_result.tools)} AWS Data Processing tools available.\")\n",
    "                print(f\"Server is ready for comprehensive data processing workflows across Glue, EMR, and Athena.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting to MCP server: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing deployed MCP server...\")\n",
    "print(\"=\" * 60)\n",
    "!uv run python3 mcp_client_remote.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
